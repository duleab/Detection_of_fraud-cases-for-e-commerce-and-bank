{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection - Model Interpretation and Feature Analysis\n",
    "\n",
    "This notebook provides comprehensive model interpretation using SHAP (SHapley Additive exPlanations) and other explainability techniques.\n",
    "\n",
    "## Objectives:\n",
    "1. Analyze feature importance across different models\n",
    "2. Generate SHAP explanations for model predictions\n",
    "3. Understand fraud patterns and decision boundaries\n",
    "4. Identify key risk factors for different fraud types\n",
    "5. Provide actionable insights for business stakeholders\n",
    "6. Validate model behavior and detect potential biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import joblib\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Model interpretation\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Utilities\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize SHAP\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "X_fraud_train = pd.read_csv('../results/X_fraud_train_scaled.csv')\n",
    "X_fraud_test = pd.read_csv('../results/X_fraud_test_scaled.csv')\n",
    "X_cc_train = pd.read_csv('../results/X_cc_train_scaled.csv')\n",
    "X_cc_test = pd.read_csv('../results/X_cc_test_scaled.csv')\n",
    "\n",
    "y_fraud_train = pd.read_csv('../results/y_fraud_train.csv').squeeze()\n",
    "y_fraud_test = pd.read_csv('../results/y_fraud_test.csv').squeeze()\n",
    "y_cc_train = pd.read_csv('../results/y_cc_train.csv').squeeze()\n",
    "y_cc_test = pd.read_csv('../results/y_cc_test.csv').squeeze()\n",
    "\n",
    "# Load feature information\n",
    "with open('../results/feature_info.pkl', 'rb') as f:\n",
    "    feature_info = pickle.load(f)\n",
    "\n",
    "def load_models_from_directory():\n",
    "    \"\"\"Load all saved models from the results directory\"\"\"\n",
    "    models = {\n",
    "        'fraud_models': {},\n",
    "        'cc_models': {}\n",
    "    }\n",
    "    \n",
    "    # Load e-commerce fraud models\n",
    "    ecom_model_files = glob.glob('../results/saved_models/ecommerce/*.pkl')\n",
    "    for model_file in ecom_model_files:\n",
    "        if 'metadata' not in model_file:\n",
    "            model_name = os.path.basename(model_file).replace('.pkl', '').replace('_ecom_20250719_043729', '')\n",
    "            try:\n",
    "                with open(model_file, 'rb') as f:\n",
    "                    models['fraud_models'][model_name] = pickle.load(f)\n",
    "                print(f\"Loaded e-commerce model: {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {model_file}: {e}\")\n",
    "    \n",
    "    # Load credit card fraud models\n",
    "    cc_model_files = glob.glob('../results/saved_models/creditcard/*.pkl')\n",
    "    for model_file in cc_model_files:\n",
    "        if 'metadata' not in model_file:\n",
    "            model_name = os.path.basename(model_file).replace('.pkl', '').replace('_cc_20250719_043729', '')\n",
    "            try:\n",
    "                with open(model_file, 'rb') as f:\n",
    "                    models['cc_models'][model_name] = pickle.load(f)\n",
    "                print(f\"Loaded credit card model: {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {model_file}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "best_models = load_models_from_directory()\n",
    "\n",
    "print(\"\\nData and models loaded successfully\")\n",
    "print(f\"Fraud features: {X_fraud_test.shape[1]}\")\n",
    "print(f\"Credit card features: {X_cc_test.shape[1]}\")\n",
    "print(f\"Available fraud models: {list(best_models['fraud_models'].keys())}\")\n",
    "print(f\"Available credit card models: {list(best_models['cc_models'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"Extract feature importance from tree-based models\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_\n",
    "        feature_imp_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance,\n",
    "            'model': model_name\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        return feature_imp_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Get feature importance for all tree-based models\n",
    "fraud_importance_results = []\n",
    "cc_importance_results = []\n",
    "\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Fraud models\n",
    "print(\"E-commerce Fraud Models:\")\n",
    "for model_name, model in best_models['fraud_models'].items():\n",
    "    importance_df = get_feature_importance(model, X_fraud_test.columns, model_name)\n",
    "    if importance_df is not None:\n",
    "        fraud_importance_results.append(importance_df)\n",
    "        print(f\"\\n{model_name} - Top 5 features:\")\n",
    "        for i, row in importance_df.head().iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Credit card models\n",
    "print(\"\\nCredit Card Fraud Models:\")\n",
    "for model_name, model in best_models['cc_models'].items():\n",
    "    importance_df = get_feature_importance(model, X_cc_test.columns, model_name)\n",
    "    if importance_df is not None:\n",
    "        cc_importance_results.append(importance_df)\n",
    "        print(f\"\\n{model_name} - Top 5 features:\")\n",
    "        for i, row in importance_df.head().iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance_results, dataset_name, top_n=15):\n",
    "    \"\"\"Plot feature importance comparison across models\"\"\"\n",
    "    if not importance_results:\n",
    "        print(f\"No tree-based models available for {dataset_name}\")\n",
    "        return\n",
    "    \n",
    "    # Combine all importance results\n",
    "    combined_df = pd.concat(importance_results, ignore_index=True)\n",
    "    \n",
    "    # Get top features across all models\n",
    "    avg_importance = combined_df.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "    top_features = avg_importance.head(top_n).index\n",
    "    \n",
    "    # Filter for top features\n",
    "    plot_df = combined_df[combined_df['feature'].isin(top_features)]\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_df = plot_df.pivot(index='feature', columns='model', values='importance')\n",
    "    pivot_df = pivot_df.reindex(top_features)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(pivot_df, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Feature Importance'})\n",
    "    plt.title(f'{dataset_name} - Feature Importance Comparison', fontweight='bold', fontsize=14)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return avg_importance\n",
    "\n",
    "# Plot feature importance for both datasets\n",
    "fraud_avg_importance = plot_feature_importance(fraud_importance_results, 'E-commerce Fraud')\n",
    "cc_avg_importance = plot_feature_importance(cc_importance_results, 'Credit Card Fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_shap_analysis(model, X_train, X_test, model_name, dataset_name, sample_size=500):\n",
    "    \"\"\"Perform SHAP analysis for model interpretation\"\"\"\n",
    "    print(f\"Performing SHAP analysis for {dataset_name} - {model_name}\")\n",
    "    \n",
    "    # Sample data for faster computation\n",
    "    if len(X_train) > sample_size:\n",
    "        train_sample = X_train.sample(n=sample_size, random_state=42)\n",
    "    else:\n",
    "        train_sample = X_train\n",
    "    \n",
    "    if len(X_test) > sample_size:\n",
    "        test_sample = X_test.sample(n=sample_size, random_state=42)\n",
    "    else:\n",
    "        test_sample = X_test\n",
    "    \n",
    "    try:\n",
    "        # Choose appropriate explainer based on model type\n",
    "        if 'RandomForest' in str(type(model)) or 'GradientBoosting' in str(type(model)):\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(test_sample)\n",
    "            # For binary classification, use positive class\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1]\n",
    "        else:\n",
    "            # Use KernelExplainer for other models (smaller sample for speed)\n",
    "            small_sample = test_sample.iloc[:100]\n",
    "            explainer = shap.KernelExplainer(model.predict_proba, train_sample.iloc[:50])\n",
    "            shap_values = explainer.shap_values(small_sample)\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1]\n",
    "            test_sample = small_sample\n",
    "        \n",
    "        return explainer, shap_values, test_sample\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in SHAP analysis for {model_name}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Perform SHAP analysis for best models\n",
    "shap_results = {}\n",
    "\n",
    "# Analyze fraud models\n",
    "print(\"\\n=== E-COMMERCE FRAUD SHAP ANALYSIS ===")\n",
    "for model_name, model in best_models['fraud_models'].items():\n",
    "    if 'random_forest' in model_name.lower() or 'gradient' in model_name.lower():\n",
    "        explainer, shap_values, test_sample = perform_shap_analysis(\n",
    "            model, X_fraud_train, X_fraud_test, model_name, 'E-commerce Fraud'\n",
    "        )\n",
    "        if shap_values is not None:\n",
    "            shap_results[f'fraud_{model_name}'] = {\n",
    "                'explainer': explainer,\n",
    "                'shap_values': shap_values,\n",
    "                'test_sample': test_sample\n",
    "            }\n",
    "            \n",
    "            # Generate summary plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.summary_plot(shap_values, test_sample, show=False)\n",
    "            plt.title(f'SHAP Summary - E-commerce Fraud ({model_name})')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            break  # Only analyze one model for demo\n",
    "\n",
    "# Analyze credit card models\n",
    "print(\"\\n=== CREDIT CARD FRAUD SHAP ANALYSIS ===")\n",
    "for model_name, model in best_models['cc_models'].items():\n",
    "    if 'random_forest' in model_name.lower() or 'gradient' in model_name.lower():\n",
    "        explainer, shap_values, test_sample = perform_shap_analysis(\n",
    "            model, X_cc_train, X_cc_test, model_name, 'Credit Card Fraud'\n",
    "        )\n",
    "        if shap_values is not None:\n",
    "            shap_results[f'cc_{model_name}'] = {\n",
    "                'explainer': explainer,\n",
    "                'shap_values': shap_values,\n",
    "                'test_sample': test_sample\n",
    "            }\n",
    "            \n",
    "            # Generate summary plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.summary_plot(shap_values, test_sample, show=False)\n",
    "            plt.title(f'SHAP Summary - Credit Card Fraud ({model_name})')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            break  # Only analyze one model for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Business Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate business insights\n",
    "print(\"\\n=== BUSINESS INSIGHTS AND RECOMMENDATIONS ===")\n",
    "print(\"\\n1. KEY FRAUD INDICATORS:\")\n",
    "\n",
    "if fraud_importance_results:\n",
    "    fraud_combined = pd.concat(fraud_importance_results, ignore_index=True)\n",
    "    top_fraud_features = fraud_combined.groupby('feature')['importance'].mean().sort_values(ascending=False).head(10)\n",
    "    print(\"\\nTop E-commerce Fraud Risk Factors:\")\n",
    "    for feature, importance in top_fraud_features.items():\n",
    "        print(f\"  - {feature}: {importance:.4f}\")\n",
    "\n",
    "if cc_importance_results:\n",
    "    cc_combined = pd.concat(cc_importance_results, ignore_index=True)\n",
    "    top_cc_features = cc_combined.groupby('feature')['importance'].mean().sort_values(ascending=False).head(10)\n",
    "    print(\"\\nTop Credit Card Fraud Risk Factors:\")\n",
    "    for feature, importance in top_cc_features.items():\n",
    "        print(f\"  - {feature}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\n2. ACTIONABLE RECOMMENDATIONS:\")\n",
    "recommendations = [\n",
    "    \"Implement real-time monitoring for high-risk features\",\n",
    "    \"Enhance verification for transactions with suspicious patterns\",\n",
    "    \"Deploy ensemble models for better fraud detection accuracy\",\n",
    "    \"Regular model retraining with new fraud patterns\",\n",
    "    \"Implement threshold optimization for business objectives\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "print(\"\\n3. MODEL DEPLOYMENT CONSIDERATIONS:\")\n",
    "deployment_notes = [\n",
    "    \"Random Forest and Gradient Boosting models show best performance\",\n",
    "    \"Credit card models achieve higher precision than e-commerce models\",\n",
    "    \"Consider ensemble approach for production deployment\",\n",
    "    \"Implement A/B testing for model performance validation\"\n",
    "]\n",
    "\n",
    "for i, note in enumerate(deployment_notes, 1):\n",
    "    print(f\"  {i}. {note}\")\n",
    "\n",
    "# Save interpretation results\n",
    "interpretation_summary = {\n",
    "    'fraud_feature_importance': fraud_importance_results,\n",
    "    'cc_feature_importance': cc_importance_results,\n",
    "    'shap_results': shap_results,\n",
    "    'business_recommendations': recommendations,\n",
    "    'deployment_notes': deployment_notes\n",
    "}\n",
    "\n",
    "with open('../results/interpretation_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(interpretation_summary, f)\n",
    "\n",
    "print(\"\\n=== ANALYSIS COMPLETE ===")\n",
    "print(\"Model interpretation results saved to ../results/interpretation_summary.pkl\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"- Feature importance analysis completed for all tree-based models\")\n",
    "print(\"- SHAP analysis provides explainable AI insights\")\n",
    "print(\"- Business recommendations generated for fraud prevention\")\n",
    "print(\"- Models ready for production deployment with proper monitoring\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
